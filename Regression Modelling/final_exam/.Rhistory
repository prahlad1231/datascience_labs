f_value <- nested_f_test$`F value`[2]
p_value <- nested_f_test$`Pr(>F)`[2]
# Print the F value and p value
cat("The nested F-test F value is:", round(f_value, 2), "and p value is", format(p_value, scientific = TRUE, digits = 2), "\n")
full_model <- lm(Fertility ~ Agriculture + Education + Infant.Mortality, data = swiss)
# Fit a reduced model without Agriculture and Education
reduced_model <- lm(Fertility ~ Infant.Mortality, data = swiss)
# Perform the nested F-test
nested_f_test <- anova(reduced_model, full_model)
# Extract the F value and p value
f_value <- nested_f_test$`F value`[2]
p_value <- nested_f_test$`Pr(>F)`[2]
# Print the F value and p value
cat("The nested F-test F value is:", round(f_value, 2), "and p value is", format(p_value, scientific = TRUE, digits = 2), "\n")
# Determine whether to reject the null hypothesis
alpha <- 0.05  # significance level
if (p_value < alpha) {
cat("We reject H0: beta_Agriculture = beta_Education = 0\n")
cat("We conclude that Agriculture and Education are jointly significant.\n")
} else {
cat("We do not reject H0: beta_Agriculture = beta_Education = 0\n")
cat("There is not enough evidence to conclude that Agriculture and Education are jointly significant.\n")
}
# Print the F value and p value
cat("The nested F-test F value is:", round(f_value, 2), "and p value is", format(p_value, scientific = TRUE, digits = 2), "\n")
# Extract the F value and p value
f_value <- nested_f_test$`F value`[2]
p_value <- nested_f_test$`Pr(>F)`[2]
# Print the F value and p value
cat("The nested F-test F value is:", round(f_value, 2), "and p value is", format(p_value, scientific = TRUE, digits = 2), "\n")
f_value
p_value
prediction_interval
lower_bound
upper_bound
# Extract the lower and upper bounds of the prediction interval
lower_bound <- prediction_interval[2]
lower_bound
upper_bound <- prediction_interval[3]
upper_bound
round(lower_bound, 2)
round(upper_bound, 2)
setwd("/Users/prahladpanthi/Documents/UC/Semester 3/Labs/Regression Modelling/final_exam")
import pandas as pd
# Load the dataset
data <- read.csv("usap.csv")
# Create an interaction term
data$interaction <- data$gr * data$pr
# Fit the regression model
model <- lm(vs ~ gr + pr + interaction, data)
# Print the summary of the regression model
summary(model)
View(data)
# Load the required library
library(rpart)
# Load the iris dataset
data(iris)
# Scale the four numerical columns
scaled_iris <- scale(iris[, 1:4])
# Build a decision tree using the first 130 rows
tree_model <- rpart(Species ~ ., data = data.frame(scaled_iris, Species = iris$Species)[1:130, ], method = "class")
# Get the tree structure
tree_structure <- summary(tree_model)
# Extract the information for the first two nodes
first_node <- tree_structure$splits$var[1]
first_threshold <- tree_structure$splits$breaks[1]
second_node <- tree_structure$splits$var[2]
second_threshold <- tree_structure$splits$breaks[2]
# Print the results
cat("The first node is", first_node, "<", first_threshold, ", and then the second node is", second_node, "<", second_threshold)
# Extract the information for the first two nodes
first_node <- tree_structure$splits$var[1]
# Load the required library
library(rpart)
# Load the iris dataset
data(iris)
# Scale the four numerical columns
scaled_iris <- scale(iris[, 1:4])
# Build a decision tree using the first 130 rows
tree_model <- rpart(Species ~ ., data = data.frame(scaled_iris, Species = iris$Species)[1:130, ], method = "class")
# Get the tree structure
tree_structure <- as.data.frame(tree_model$splits)
# Extract the information for the first two nodes
first_node <- unique(tree_structure$variable)[1]
first_threshold <- tree_structure$split[tree_structure$variable == first_node][1]
second_node <- unique(tree_structure$variable)[2]
second_threshold <- tree_structure$split[tree_structure$variable == second_node][1]
# Print the results
cat("The first node is", first_node, "<", first_threshold, ", and then the second node is", second_node, "<", second_threshold)
# Load the required library
library(rpart)
# Load the iris dataset
data(iris)
# Scale the four numerical columns
scaled_iris <- scale(iris[, 1:4])
# Build a decision tree using the first 130 rows
tree_model <- rpart(Species ~ ., data = data.frame(scaled_iris, Species = iris$Species)[1:130, ], method = "class")
# Get the tree structure
tree_structure <- as.data.frame(tree_model$frame)
# Extract the information for the first two nodes
first_node <- tree_structure$var[1]
first_threshold <- tree_structure$yval[1]
second_node <- tree_structure$var[2]
second_threshold <- tree_structure$yval[2]
# Print the results
cat("The first node is", first_node, "<", first_threshold, ", and then the second node is", second_node, "<", second_threshold)
# Load the required library
library(rpart)
# Load the iris dataset
data(iris)
# Scale the four numerical columns
scaled_iris <- scale(iris[, 1:4])
scaled_iris
# Build a decision tree using the first 130 rows
tree_model <- rpart(Species ~ ., data = data.frame(scaled_iris, Species = iris$Species)[1:130, ], method = "class")
# Extract information for the first two nodes
first_node <- names(tree_structure$variable[1])
first_threshold <- round(tree_structure$breaks[1], 3)
# Extract the variable names and threshold values for the first two nodes
first_split <- tree_model$frame[1, ]
second_split <- tree_model$frame[2, ]
# Print the results
cat("The first node is", first_split$var, "<", formatC(first_split$yval, digits = 3),
", and then the second node is", second_split$var, "<", formatC(second_split$yval, digits = 3))
colnames(iris)
# Load the required library
library(mlbench)
# Load the PimaIndiansDiabetes2 dataset and remove missing values
data(PimaIndiansDiabetes2)
diabetes_data <- na.omit(PimaIndiansDiabetes2)
# Load the PimaIndiansDiabetes2 dataset and remove missing values
data(PimaIndiansDiabetes2)
# Load the required library
library(mlbench)
# Load the required library
install.packages("mlbench")
library(mlbench)
# Load the PimaIndiansDiabetes2 dataset and remove missing values
data(PimaIndiansDiabetes2)
diabetes_data <- na.omit(PimaIndiansDiabetes2)
# Fit a logistic model using age as the predictor
model_age <- glm(diabetes ~ age, data = diabetes_data, family = binomial)
# Calculate AIC for the model
aic_age <- AIC(model_age)
# Fit a logistic model using age, mass, and pressure as predictors
model_age_mass_pressure <- glm(diabetes ~ age + mass + pressure, data = diabetes_data, family = binomial)
# Calculate AIC for the model
aic_age_mass_pressure <- AIC(model_age_mass_pressure)
# Create data for prediction
new_data <- data.frame(age = 26, mass = 31, pressure = 50)
# Use the better model to predict the probability for the new data
if (aic_age < aic_age_mass_pressure) {
predicted_prob <- predict(model_age, newdata = new_data, type = "response")
} else {
predicted_prob <- predict(model_age_mass_pressure, newdata = new_data, type = "response")
}
# Round AIC values and predicted probability to two decimal places
aic_age <- round(aic_age, 2)
aic_age_mass_pressure <- round(aic_age_mass_pressure, 2)
predicted_prob <- round(predicted_prob, 2)
# Print the results
cat("(1) AIC value with age as the predictor:", aic_age, "\n")
cat("(2) AIC value with age, mass, and pressure as predictors:", aic_age_mass_pressure, "\n")
cat("(3) Predicted probability to have diabetes:", predicted_prob, "\n")
# Load the required library and load the dataset
library(faraway)
# Load the required library and load the dataset
install.packages("faraway")
library(faraway)
data(gala)
# Subset the data to the first 23 rows
gala_subset <- gala[1:23, ]
# Fit a Poisson GLM with mu_i as the mean of Species_i by Endemics_i
model1 <- glm(Species ~ Endemics, data = gala_subset, family = poisson)
# Calculate AIC for the first model
aic1 <- AIC(model1)
# Fit a Poisson GLM with mu_i as the mean of Species_i by Endemics_i, Area_i, and Nearest_i
model2 <- glm(Species ~ Endemics + Area + Nearest, data = gala_subset, family = poisson)
# Calculate AIC for the second model
aic2 <- AIC(model2)
# Round AIC values to three decimal places
aic1 <- round(aic1, 3)
aic2 <- round(aic2, 3)
# Print the results
cat("(1) fitted_log_mu_i =", round(coef(model1)[1], 3), "+", round(coef(model1)[2], 3), " * Endemics_i\n")
cat("    to obtain the first AIC =", aic1, "\n")
cat("(2) The second AIC =", aic2, "which is smaller than the first AIC, so we conclude that the model in (2) is better than the model in (1).\n")
# Load the required library and load the dataset
library(faraway)
data(motorins)
# Subset the data to the first 45 rows
motorins_subset <- motorins[1:45, ]
# Fit a gamma GLM with mu_i as the mean of perd_i by Bonus_i
model1 <- glm(perd ~ Bonus, data = motorins_subset, family = Gamma(link = "log"))
# Calculate AIC for the first model
aic1 <- AIC(model1)
# Fit a gamma GLM with mu_i as the mean of perd_i by Bonus_i, Claims_i, and Insured_i
model2 <- glm(perd ~ Bonus + Claims + Insured, data = motorins_subset, family = Gamma(link = "log"))
# Calculate AIC for the second model
aic2 <- AIC(model2)
# Perform the chi-square test
deviance_diff <- anova(model1, model2, test = "Chisq")$Deviance[2]
p_value <- 1 - pchisq(deviance_diff, df = 3)  # Degrees of freedom = 3
# Round coefficients, AIC values, and p-value to specified decimal places
coef_model1 <- coef(model1)
coef_model2 <- coef(model2)
fitted_log_mu_1 <- round(coef_model1[1], 2)
fitted_log_mu_2 <- round(coef_model2[1], 2)
coef_bonus_1 <- round(coef_model1[2], 2)
coef_bonus_2 <- round(coef_model2[2], 2)
coef_claims_2 <- round(coef_model2[3], 2)
coef_insured_2 <- round(coef_model2[4], 3)
aic1 <- round(aic1, 2)
aic2 <- round(aic2, 2)
deviance_diff <- round(deviance_diff, 2)
p_value <- round(p_value, 3)
# Print the results
cat("(1) fitted_log_mu_i =", fitted_log_mu_1, "+", coef_bonus_1, " * Bonus_i\n")
cat("    with AIC =", aic1, "\n")
cat("(2) fitted_log_mu_i =", fitted_log_mu_2, "+", coef_bonus_2, " * Bonus_i +", coef_claims_2, "E-04 * Claims_i +", coef_insured_2, "E-05 * Insured_i\n")
cat("    with AIC =", aic2, "\n")
cat("(3) Chi-square test statistic (deviance) =", deviance_diff, "and p-value =", p_value, "\n")
cat("    which is not smaller than 0.05, so we cannot reject H0. We conclude that model in (1) is better than model in (2).\n")
p_value <- round(p_value, 2)
cat("(3) Chi-square test statistic (deviance) =", deviance_diff, "and p-value =", p_value, "\n")
cat("    which is not smaller than 0.05, so we cannot reject H0. We conclude that model in (1) is better than model in (2).\n")
# Load the glmnet library
library(glmnet)
# Prepare the data for glmnet
x_train <- as.matrix(training_set[, -6])  # Exclude the "Year" variable
y_train <- training_set$Employed
# Load the longley dataset
data(longley)
# Split the data into a training set (first 12 observations) and a test set (remaining observations)
training_set <- longley[1:12, ]
test_set <- longley[13:16, ]
# Load the glmnet library
library(glmnet)
# Prepare the data for glmnet
x_train <- as.matrix(training_set[, -6])  # Exclude the "Year" variable
y_train <- training_set$Employed
x_test <- as.matrix(test_set[, -6])  # Exclude the "Year" variable
y_test <- test_set$Employed
# Fit a Lasso model using glmnet with lambda = 0.01
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = 0.01)
# Predict Employed on the test set
lasso_pred <- predict(lasso_model, newx = x_test)
# Calculate RMSE and MAE
rmse <- sqrt(mean((lasso_pred - y_test)^2))
mae <- mean(abs(lasso_pred - y_test))
# Predict Employed for the year 1959
x_1959 <- as.matrix(longley[13, -6])  # Data for the year 1959
employed_1959 <- predict(lasso_model, newx = x_1959)
# Calculate residuals
residuals <- as.vector(lasso_pred - y_test)
sum_residuals <- sum(residuals[2:5])  # Residuals for the years 1959-1962
# Print the results with 2 decimal places
cat("The estimated intercept is", round(lasso_model$a0, 2), "\n")
cat("The estimated slope for Population is", round(coef(lasso_model)[7], 2), "\n")
cat("The RMSE value for the fitted model on the test set is", round(rmse, 2), "\n")
cat("The MAE value for the fitted model on the test set is", round(mae, 2), "\n")
cat("The predicted value of Employed for the year 1959 is", round(employed_1959, 2), "\n")
cat("The residual for the year 1959 is", round(residuals[1], 2), "\n")
cat("The sum of the residual values for the years 1959-1962 is", round(sum_residuals, 2), "\n")
sum_residuals
sum_residuals <- sum(residuals[2:5])  # Residuals for the years 1959-1962
sum_residuals
p_value <- round(p_value, 2)
p_value
# Load required library
library(glmnet)
# Load the longley dataset
data(longley)
# Define the predictors and response variable
predictors <- longley[,c("GNP.deflator", "GNP", "Unemployed", "Armed.Forces", "Population", "Year")]
response <- longley[,"Employed"]
# Split the data into training and test sets
train_data <- predictors[1:12,]
test_data <- predictors[13:16,]
train_labels <- response[1:12]
test_labels <- response[13:16]
# Fit the lasso model
set.seed(123)  # for reproducibility
cv_fit <- cv.glmnet(as.matrix(train_data), train_labels, alpha = 1, lambda = 0.01)
# Get the coefficients
coefs <- coef(cv_fit, s = "lambda.min")
cv_fit <- cv.glmnet(as.matrix(train_data), train_labels, alpha = 1, lambda = 0.01)
# Load the dataset
data <- read.csv("usap.csv")
# Create an interaction term
data$interaction <- data$gr * data$pr
# Fit the regression model
model <- lm(vs ~ gr + pr + interaction, data)
# Print the summary of the regression model
summary(model)
# Load the required library
library(glmnet)
# Load the Longley dataset
data(longley)
# Use the first 12 observations as your training set and the remaining as your test set
train <- longley[1:12,]
test <- longley[13:16,]
# Fit a lasso model using glmnet
x_train <- model.matrix(Employed~., data = train)[,-1]
y_train <- train$Employed
x_test <- model.matrix(Employed~., data = test)[,-1]
y_test <- test$Employed
fit <- glmnet(x_train, y_train, alpha = 1, lambda = 0.01)
# Get the coefficients
coef(fit)
# Predict on the test set
predictions <- predict(fit, newx = x_test)
# Calculate RMSE
rmse <- sqrt(mean((y_test - predictions)^2))
# Calculate MAE
mae <- mean(abs(y_test - predictions))
# Predict for the year 1959
pred_1959 <- predict(fit, newx = x_test[which(test$Year == 1959),])
# Calculate the residual for 1959
residual_1959 <- y_test[which(test$Year == 1959)] - pred_1959
# Calculate the sum of residuals for the years 1959-1962
sum_residuals <- sum(y_test - predictions)
# Get the coefficients
coef(fit)
# Load the required library
library(rpart)
# Load the iris dataset
data(iris)
# Scale the four numerical columns
iris[,1:4] <- scale(iris[,1:4])
# Use the first 130 rows to build a decision tree
fit <- rpart(Species ~ ., data = iris[1:130,], method = "class")
# Print the decision tree
printcp(fit)
# Plot the decision tree
plotcp(fit)
# Load the required library
library(glmnet)
# Load the Longley dataset
data(longley)
# Use the first 12 observations as your training set and the remaining as your test set
train <- longley[1:12,]
test <- longley[13:16,]
# Fit a lasso model using glmnet
x_train <- model.matrix(Employed~., data = train)[,-1]
y_train <- train$Employed
x_test <- model.matrix(Employed~., data = test)[,-1]
y_test <- test$Employed
fit <- glmnet(x_train, y_train, alpha = 1, lambda = 0.01)
# Get the coefficients
coef(fit)
# Predict on the test set
predictions <- predict(fit, newx = x_test)
# Calculate RMSE
rmse <- sqrt(mean((y_test - predictions)^2))
# Calculate MAE
mae <- mean(abs(y_test - predictions))
# Predict for the year 1959
pred_1959 <- predict(fit, newx = x_test[which(test$Year == 1959),])
# Calculate the residual for 1959
residual_1959 <- y_test[which(test$Year == 1959)] - pred_1959
# Calculate the sum of residuals for the years 1959-1962
sum_residuals <- sum(y_test - predictions)
sum_residuals
residual_1959
pred_1959
mae
rmse
predictions
# Get the coefficients
coef(fit)
# Load the longley data set
data(longley)
# Split the data into training and test sets
train <- longley[1:12, ]
test <- longley[13:16, ]
# Fit a lasso model using glmnet
mod <- glmnet(train$Employed ~ train[, 2:7], family = "gaussian", alpha = 1, lambda = 0.01)
# Print the summary of the regression model
summary(model)
# Load the required library and load the dataset
library(faraway)
data(motorins)
# Subset the data to the first 45 rows
motorins_subset <- motorins[1:45, ]
# Fit a gamma GLM with mu_i as the mean of perd_i by Bonus_i
model1 <- glm(perd ~ Bonus, data = motorins_subset, family = Gamma(link = "log"))
# Calculate AIC for the first model
aic1 <- AIC(model1)
# Fit a gamma GLM with mu_i as the mean of perd_i by Bonus_i, Claims_i, and Insured_i
model2 <- glm(perd ~ Bonus + Claims + Insured, data = motorins_subset, family = Gamma(link = "log"))
# Calculate AIC for the second model
aic2 <- AIC(model2)
# Perform the chi-square test
deviance_diff <- anova(model1, model2, test = "Chisq")$Deviance[2]
p_value <- 1 - pchisq(deviance_diff, df = 3)  # Degrees of freedom = 3
# Round coefficients, AIC values, and p-value to specified decimal places
coef_model1 <- coef(model1)
coef_model2 <- coef(model2)
fitted_log_mu_1 <- round(coef_model1[1], 2)
fitted_log_mu_2 <- round(coef_model2[1], 2)
coef_bonus_1 <- round(coef_model1[2], 2)
coef_bonus_2 <- round(coef_model2[2], 2)
coef_claims_2 <- round(coef_model2[3], 2)
coef_insured_2 <- round(coef_model2[4], 3)
aic1 <- round(aic1, 2)
aic2 <- round(aic2, 2)
deviance_diff <- round(deviance_diff, 2)
p_value <- round(p_value, 2)
p_value
# Print the results
cat("(1) fitted_log_mu_i =", fitted_log_mu_1, "+", coef_bonus_1, " * Bonus_i\n")
cat("    with AIC =", aic1, "\n")
cat("(2) fitted_log_mu_i =", fitted_log_mu_2, "+", coef_bonus_2, " * Bonus_i +", coef_claims_2, "E-04 * Claims_i +", coef_insured_2, "E-05 * Insured_i\n")
cat("    with AIC =", aic2, "\n")
cat("(3) Chi-square test statistic (deviance) =", deviance_diff, "and p-value =", p_value, "\n")
cat("    which is not smaller than 0.05, so we cannot reject H0. We conclude that model in (1) is better than model in (2).\n")
# Perform the chi-square test
deviance_diff <- anova(model1, model2, test = "Chisq")$Deviance[2]
# Perform the chi-square test
deviance_diff <- anova(model1, model2, test = "Chisq")$Deviance[2]
p_value <- 1 - pchisq(deviance_diff, df = 2)  # Degrees of freedom = 3
p_value
# Round coefficients, AIC values, and p-value to specified decimal places
coef_model1 <- coef(model1)
coef_model2 <- coef(model2)
fitted_log_mu_1 <- round(coef_model1[1], 2)
fitted_log_mu_2 <- round(coef_model2[1], 2)
coef_bonus_1 <- round(coef_model1[2], 2)
coef_bonus_2 <- round(coef_model2[2], 2)
coef_claims_2 <- round(coef_model2[3], 2)
coef_insured_2 <- round(coef_model2[4], 3)
aic1 <- round(aic1, 2)
aic2 <- round(aic2, 2)
deviance_diff <- round(deviance_diff, 2)
p_value <- round(p_value, 2)
p_value
# Subset the data to the first 45 rows
motorins_subset <- motorins[1:45, ]
ncol(motorins_subset)
p_value <- 1 - pchisq(deviance_diff, df = 308)  # Degrees of freedom = 3
p_value
p_value <- 1 - pchisq(deviance_diff, df = 308)  # Degrees of freedom = 3
p_value
p_value <- 1 - pchisq(deviance_diff, df = 3)  # Degrees of freedom = 3
p_value
# Load the longley data set
# Load the longley dataset
data(longley)
# Split the data into a training set (first 12 observations) and a test set (remaining observations)
training_set <- longley[1:12, ]
test_set <- longley[13:16, ]
# Load the glmnet library
library(glmnet)
# Prepare the data for glmnet
x_train <- as.matrix(training_set[, -6])  # Exclude the "Year" variable
y_train <- training_set$Employed
x_test <- as.matrix(test_set[, -6])  # Exclude the "Year" variable
y_test <- test_set$Employed
# Fit a Lasso model using glmnet with lambda = 0.01
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = 0.01)
# Predict Employed on the test set
lasso_pred <- predict(lasso_model, newx = x_test)
# Calculate RMSE and MAE
rmse <- sqrt(mean((lasso_pred - y_test)^2))
mae <- mean(abs(lasso_pred - y_test))
# Predict Employed for the year 1959
x_1959 <- as.matrix(longley[13, -6])  # Data for the year 1959
employed_1959 <- predict(lasso_model, newx = x_1959)
# Calculate residuals
residuals <- as.vector(lasso_pred - y_test)
sum_residuals <- sum(residuals)
# Print the results with 2 decimal places
cat("The estimated intercept is", round(lasso_model$a0, 2), "\n")
cat("The estimated slope for Population is", round(coef(lasso_model)[6], 2), "\n")
cat("The RMSE value for the fitted model on the test set is", round(rmse, 2), "\n")
cat("The MAE value for the fitted model on the test set is", round(mae, 2), "\n")
cat("The predicted value of Employed for the year 1959 is", round(employed_1959, 2), "\n")
cat("The residual for the year 1959 is", round(residuals[1], 2), "\n")
cat("The sum of the residual values for the 4 years 1959-1962 is", round(sum_residuals, 2), "\n")
# Load the required library
library(glmnet)
# Load the Longley dataset
data(longley)
# Define the predictors and the response variable
predictors <- longley[,c("GNP.deflator", "GNP", "Unemployed", "Armed.Forces", "Population", "Year")]
response <- longley[,"Employed"]
# Split the data into training and test sets
train_data <- predictors[1:12,]
test_data <- predictors[13:16,]
train_labels <- response[1:12]
test_labels <- response[13:16]
# Fit the lasso model
fit <- glmnet(as.matrix(train_data), train_labels, alpha = 1, lambda = 0.01)
# Get the coefficients
coef(fit)
# Predict on the test set
predictions <- predict(fit, newx = as.matrix(test_data))
# Calculate RMSE
rmse <- sqrt(mean((predictions - test_labels)^2))
# Calculate MAE
mae <- mean(abs(predictions - test_labels))
# Calculate residuals for 1959
residual_1959 <- test_labels["1959"] - predictions["1959"]
# Calculate sum of residuals for 1959-1962
sum_residuals <- sum(test_labels - predictions)
sum_residuals
residual_1959
mae
rmse
predictors
predictions
